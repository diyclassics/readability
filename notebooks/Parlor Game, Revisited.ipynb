{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parlor Game, Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to accompany a Disiecta Membra blog post about the DCC Core Latin Vocabulary and the language of Virgil's *Aeneid*. This post can be found here: \n",
    "\n",
    "Source for .csv file in /data:\n",
    "Francese, Christopher. Latin Core Vocabulary. Dickinson College Commentaries (2014). http://dcc.dickinson.edu/latin-vocabulary-list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "\n",
    "datapath = 'data/'\n",
    "datafile = \"latin_vocabulary_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up training sentences\n",
    "\n",
    "rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "path = os.path.expanduser(rel_path)\n",
    "\n",
    "# Check for presence of latin_pos_lemmatized_sents\n",
    "file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "\n",
    "latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "    latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "else:\n",
    "    latin_pos_lemmatized_sents = []\n",
    "    print('The file %s is not available in cltk_data' % file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up NLP tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: DCC Core Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load first column of DCC Core Vocabulary csv file\n",
    "\n",
    "columns = defaultdict(list)\n",
    "\n",
    "with open(datapath+datafile) as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        for (i,v) in enumerate(row):\n",
    "            columns[i].append(v)\n",
    "\n",
    "dcc_lemmas = columns[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split headword column by whitespace and keep only first word\n",
    "\n",
    "dcc_lemmas_simple = [lemma.replace('/',' ').split()[0] for lemma in dcc_lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess DCC lemmas\n",
    "\n",
    "# Normalize u/v\n",
    "dcc_lemmas_simple = [replacer.replace(lemma) for lemma in dcc_lemmas_simple]\n",
    "\n",
    "# remove macrons\n",
    "def remove_macrons(text):\n",
    "    transmap = {ord('ā'): 'a', ord('ē'): 'e', ord('ī'): 'i', ord('ō'): 'o', ord('ū'): 'u', }\n",
    "    return text.translate(transmap)\n",
    "\n",
    "dcc_lemmas_simple = [remove_macrons(lemma) for lemma in dcc_lemmas_simple]\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "dcc_lemmas_simple = [lemma.translate(translator) for lemma in dcc_lemmas_simple]\n",
    "\n",
    "dcc_lemmas_simple.sort()  \n",
    "\n",
    "print(len(dcc_lemmas_simple))\n",
    "print(dcc_lemmas_simple[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Aeneid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Aeneid text from Latin Library corpus\n",
    "\n",
    "files = latinlibrary.fileids()\n",
    "aeneid_files = [file for file in files if 'vergil/aen' in file]\n",
    "aeneid_raw = latinlibrary.raw(aeneid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess Aeneid text\n",
    "\n",
    "# Remove English paratexts from Latin Library texts\n",
    "aeneid_text = re.sub(r'Vergil: Aeneid .{,2}', '', aeneid_raw)\n",
    "aeneid_text = re.sub(r'P. VERGILI MARONIS AENEIDOS LIBER \\b.+?\\b', '', aeneid_text)\n",
    "aeneid_text = re.sub(r'\\bVergil\\b','',aeneid_text)\n",
    "aeneid_text = re.sub(r'\\bThe Latin Library\\b','',aeneid_text)\n",
    "aeneid_text = re.sub(r'\\bThe Classics Page\\b','',aeneid_text)\n",
    "aeneid_text = aeneid_text.replace('&#151;', ' ')\n",
    "\n",
    "# Lowercase\n",
    "aeneid_text = aeneid_text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "aeneid_text  = aeneid_text.translate(translator)\n",
    "\n",
    "# Remove numbers\n",
    "translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "aeneid_text  = aeneid_text.translate(translator)\n",
    "\n",
    "# Normalize u/v\n",
    "aeneid_text = replacer.replace(aeneid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize Aeneid\n",
    "\n",
    "tokens = tokenizer.tokenize(aeneid_text)\n",
    "lemmas = lemmatizer.lemmatize(tokens)\n",
    "\n",
    "# Lemmatizer returns a list of tuples in the form [(token, lemma)]\n",
    "# Keep only the lemmas\n",
    "aeneid_lemmas = [lemma[1] for lemma in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Postprocess Aeneid lemmas\n",
    "\n",
    "# Some lemmas are returned with Morpheus number, e.g. accido1\n",
    "# We want to remove these numbers because they are not used in\n",
    "# the DCC core vocabulary.\n",
    "translator = str.maketrans({key: \"\" for key in '0123456789'})\n",
    "aeneid_lemmas = [lemma.translate(translator) for lemma in aeneid_lemmas]\n",
    "\n",
    "# Normalize u/v in the output\n",
    "aeneid_lemmas = [replacer.replace(lemma) for lemma in aeneid_lemmas]\n",
    "\n",
    "print(aeneid_lemmas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of words reported in the DCC blog post\n",
    "\n",
    "dcc_missing = ['aegre', 'arbitror', 'auctoritas', 'beneficium', 'celeriter', 'censeo', 'ceterum', 'cibus', 'cogito', 'comparo', 'consuetudo', 'damnum', 'desidero', 'dignitas', 'disciplina', 'dormio', 'eo', 'epistula', 'existimo', 'fabula', 'facinus', 'familia', 'fructus', 'imperator', 'initium', 'intellego', 'interficio', 'interim', 'interrogo', 'itaque', 'iudico', 'libido', 'littera', 'magnitudo', 'maiores', 'materia', 'memoria', 'multitudo', 'mundus', 'necessitas', 'negotium', 'nolo', 'oportet', 'oratio', 'paene', 'pecunia', 'pertineo', 'plerumque', 'plerusque', 'poeta', 'postea', 'praetor', 'priuatus', 'prouincia', 'publicus', 'quasi', 'quemadmodum', 'quidam', 'reliquus', 'reuerto', 'sapiens', 'sapientia', 'scientia', 'scribo', 'seruus', 'solum', 'statim', 'studeo', 'tamquam', 'tribunus', 'uagus', 'uitium', 'utilis', 'utrum', 'uxor']\n",
    "\n",
    "print(len(dcc_missing))\n",
    "print(dcc_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the list of missing DCC words from the Aeneid text generated by the CLTK Backoff Latin lemmatizer\n",
    "\n",
    "cltk_missing = list(set([lemma for lemma in dcc_lemmas_simple if lemma not in aeneid_lemmas]))\n",
    "cltk_missing.sort()\n",
    "\n",
    "print(len(cltk_missing))\n",
    "print(cltk_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that fail to match for technical reasons:\n",
    "# - CLTK Backoff Latin lemmatizer (by default in beta version) distinguishes between\n",
    "#    cum/with and cum/when as cum1 and cum2; the match here fails because of the\n",
    "#    number, not the method\n",
    "# - CLTK lemmatizer also appends a hyphen to enclitics, e.g. '-que' which prevents\n",
    "#   matching\n",
    "# \n",
    "\n",
    "technical_forms = ['cum', 'que', 'ue']\n",
    "cltk_missing = [lemma for lemma in cltk_missing if lemma not in technical_forms]\n",
    "print(len(cltk_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that fail because of alternative forms:\n",
    "# - The CLTK lemmatizer defers to the practice of the »Ancient Greek and Latin\n",
    "#   Dependency Treebank«) [https://perseusdl.github.io/treebank_data/] and uses\n",
    "#   this data as the source of its default training data.\n",
    "#   Form some words here, AGLDT uses a different base form:\n",
    "#   - e.g. AGLDT uses 'atque' as the base form; DCC uses 'ac'\n",
    "\n",
    "alt_forms  = ['a', 'ac', 'nec']\n",
    "cltk_missing = [lemma for lemma in cltk_missing if lemma not in alt_forms]\n",
    "print(len(cltk_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which DCC missing lemmas did the CLTK lemmatizer correctly match (true positive)?\n",
    "\n",
    "tp = [lemma for lemma in cltk_missing if lemma in dcc_missing]\n",
    "print(len(tp))\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which DCC missing lemmas did the CLTK lemmatizer miss (false negative)?\n",
    "\n",
    "fn = [lemma for lemma in dcc_missing if lemma not in cltk_missing]\n",
    "print(len(fn))\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which lemmas did the CLTK lemmatizer return that were not on the DCC list (false positive)?\n",
    "\n",
    "fp = [lemma for lemma in cltk_missing if lemma not in dcc_missing]\n",
    "print(len(fp))\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat study for Lucan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize Lucan\n",
    "\n",
    "tokens = tokenizer.tokenize(lucan_text)\n",
    "lemmas = lemmatizer.lemmatize(tokens)\n",
    "\n",
    "# Lemmatizer returns a list of tuples in the form [(token, lemma)]\n",
    "# Keep only the lemmas\n",
    "lucan_lemmas = [lemma[1] for lemma in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Postprocess Lucan lemmas\n",
    "\n",
    "# Some lemmas are returned with Morpheus number, e.g. accido1\n",
    "# We want to remove these numbers because they are not used in\n",
    "# the DCC core vocabulary.\n",
    "translator = str.maketrans({key: \"\" for key in '0123456789'})\n",
    "lucan_lemmas = [lemma.translate(translator) for lemma in lucan_lemmas]\n",
    "\n",
    "# Normalize u/v in the output\n",
    "lucan_lemmas = [replacer.replace(lemma) for lemma in lucan_lemmas]\n",
    "\n",
    "print(lucan_lemmas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the list of missing DCC words from the Aeneid text generated by the CLTK Backoff Latin lemmatizer\n",
    "\n",
    "cltk_missing_lucan = list(set([lemma for lemma in dcc_lemmas_simple if lemma not in lucan_lemmas]))\n",
    "cltk_missing_lucan.sort()\n",
    "\n",
    "print(len(cltk_missing_lucan))\n",
    "print(cltk_missing_lucan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that fail to match for technical reasons:\n",
    "# - CLTK Backoff Latin lemmatizer (by default in beta version) distinguishes between\n",
    "#    cum/with and cum/when as cum1 and cum2; the match here fails because of the\n",
    "#    number, not the method\n",
    "# - CLTK lemmatizer also appends a hyphen to enclitics, e.g. '-que' which prevents\n",
    "#   matching\n",
    "# \n",
    "\n",
    "technical_forms = ['cum', 'que', 'ue']\n",
    "cltk_missing_lucan = [lemma for lemma in cltk_missing_lucan if lemma not in technical_forms]\n",
    "print(len(cltk_missing_lucan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that fail because of alternative forms:\n",
    "# - The CLTK lemmatizer defers to the practice of the »Ancient Greek and Latin\n",
    "#   Dependency Treebank«) [https://perseusdl.github.io/treebank_data/] and uses\n",
    "#   this data as the source of its default training data.\n",
    "#   Form some words here, AGLDT uses a different base form:\n",
    "#   - e.g. AGLDT uses 'atque' as the base form; DCC uses 'ac'\n",
    "\n",
    "alt_forms  = ['a', 'ac', 'nec']\n",
    "cltk_missing_lucan = [lemma for lemma in cltk_missing_lucan if lemma not in alt_forms]\n",
    "print(len(cltk_missing_lucan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cltk_missing_lucan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([lemma for lemma in cltk_missing_lucan if lemma not in cltk_missing])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
